{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "version": "3.6.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "name": "python",
      "mimetype": "text/x-python"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Quora Binary Classification\n",
        "\n",
        "\n",
        "This is a binary classification (traditional)/semantic classification approach by applying different techniques -EDA(feature engineering,gram analysis),embeddings,deep neural networks,transformer architectures and evaluation.\n",
        "\n"
      ],
      "metadata": {
        "id": "djlsBhSCWwFc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# EDA- Analysis and Benchmarking\n",
        "\n"
      ],
      "metadata": {
        "id": "Ls__qr5JWwFc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation of the Dataset\n",
        "\n",
        "\n",
        "1. [PCA](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html),[TSNE](https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html),[SVD](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.TruncatedSVD.html)\n",
        "2. [tfidfvectorization](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html),[CountVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html)\n",
        "3. [WordCloud](https://pypi.org/project/wordcloud/)\n",
        "4. [Gram-Analysis](https://www.nltk.org/)\n",
        "5. [Eda-notebook](https://www.kaggle.com/abhilash1910/tweet-analysis-eda-cleaning-tsne-glove-tf)\n",
        "\n",
        "\n",
        "PCA works by eigenvector decomposition strategy as shown in the sample:\n",
        "\n",
        "<img src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_ica_vs_pca_thumb.png\"></img>\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "l61uqg_NWwFd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load\n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "# Input data files are available in the read-only \"../input/\" directory\n",
        "# For example, running this (by cli cking run or pressing Shift+Enter) will list all files under the input directory\n",
        "\n",
        "import os\n",
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))\n",
        "\n",
        "# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\"\n",
        "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "id": "NOgP7U0VWwFd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "!ls ../input\n",
        "!ls ../input/quora-insincere-questions-classification"
      ],
      "metadata": {
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O2lVBzObWwFd",
        "outputId": "68c38226-e8b9-4168-dca0-6942db77e733"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ls: cannot access '../input': No such file or directory\n",
            "ls: cannot access '../input/quora-insincere-questions-classification': No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#import libraries useful for the entire module\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import xgboost as xgb\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as mpatches\n",
        "import matplotlib\n",
        "from tqdm import tqdm\n",
        "from sklearn.svm import SVC\n",
        "from keras.models import Sequential\n",
        "from keras.layers.recurrent import LSTM, GRU\n",
        "from keras.layers.core import Dense, Activation, Dropout\n",
        "from keras.layers.embeddings import Embedding\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "from keras.utils import np_utils\n",
        "from sklearn import preprocessing, decomposition, model_selection, metrics, pipeline\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from keras.layers import GlobalMaxPooling1D, Conv1D, MaxPooling1D, Flatten, Bidirectional, SpatialDropout1D\n",
        "from keras.preprocessing import sequence, text\n",
        "from keras.callbacks import EarlyStopping\n",
        "from nltk import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "stop_words = stopwords.words('english')\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as mpatches\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.decomposition import PCA, TruncatedSVD,SparsePCA\n",
        "from sklearn.metrics import classification_report,confusion_matrix\n",
        "from nltk.tokenize import word_tokenize\n",
        "from collections import defaultdict\n",
        "from collections import Counter\n",
        "import seaborn as sns\n",
        "from wordcloud import WordCloud,STOPWORDS\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "import string\n",
        "from plotly import tools\n",
        "import plotly.offline as py\n",
        "py.init_notebook_mode(connected=True)\n",
        "import plotly.graph_objs as go"
      ],
      "metadata": {
        "trusted": true,
        "id": "q800QUtfWwFe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import preprocessing,metrics,manifold\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.model_selection import train_test_split,cross_val_score,GridSearchCV,cross_val_predict\n",
        "from imblearn.over_sampling import ADASYN,SMOTE\n",
        "from imblearn.under_sampling import NearMiss\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "from sklearn.svm import SVC\n",
        "import collections\n",
        "import keras as k\n",
        "import matplotlib.patches as mpatches\n",
        "from sklearn.metrics import accuracy_score\n",
        "%matplotlib inline\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "import xgboost\n",
        "from imblearn.metrics import classification_report_imbalanced\n",
        "from sklearn.metrics import classification_report,roc_auc_score,roc_curve,r2_score,recall_score,confusion_matrix,precision_recall_curve\n",
        "from collections import Counter\n",
        "from sklearn.model_selection import StratifiedKFold,KFold,StratifiedShuffleSplit\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "8GgJCNpIWwFf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df=pd.read_csv('../input/quora-insincere-questions-classification/train.csv')\n",
        "test_df=pd.read_csv('../input/quora-insincere-questions-classification/test.csv')\n",
        "print(train_df.head())\n",
        "print(test_df.head())\n",
        "print(\"===========\")\n",
        "print(\"Training Shape\".format(),train_df.shape)\n",
        "print(\"Testing Shape\".format(),test_df.shape)"
      ],
      "metadata": {
        "trusted": true,
        "id": "i6dmNSQDWwFf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"The type of columns in the dataset\")\n",
        "print(\"Columns\".format(),train_df.columns)"
      ],
      "metadata": {
        "trusted": true,
        "id": "8X7p2mSpWwFf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#Insincere question test\n",
        "train_ext=train_df[train_df['target']==1]['question_text']\n",
        "print(train_ext)\n",
        "\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "I4Mm_zzXWwFf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Analysis 1- Word statistics\n",
        "\n",
        "This analysis would be done on both the training and testing datasets.\n",
        "A fundamental part of any NLP pipeline before applying transformations"
      ],
      "metadata": {
        "id": "IR9BJQqFWwFf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Training data statistics\n",
        "\n",
        "#Estimate the value counts\n",
        "count_types=train_df['target'].value_counts()\n",
        "print(\"Extracting counts\".format(),count_types)\n",
        "#Count targets with value 1\n",
        "\n",
        "count_ones=train_df[train_df['target']==1].shape[0]\n",
        "print(count_ones)\n",
        "count_zeros=train_df[train_df['target']==0].shape[0]\n",
        "print(count_zeros)\n",
        "\n",
        "#Matplot to plot the amount of questions from either types\n",
        "def plot_counts(count_ones,count_zeros):\n",
        "    plt.rcParams['figure.figsize']=(6,6)\n",
        "    plt.bar(0,count_ones,width=0.6,label='InSincere Questions',color='Red')\n",
        "    plt.legend()\n",
        "    plt.bar(2,count_zeros,width=0.6,label='Sincere Questions',color='Green')\n",
        "    plt.legend()\n",
        "    plt.ylabel('Count of Questions (in M)')\n",
        "    plt.xlabel('Types of Questions')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "#Seaborn Dist plot for analysing the length of a question sentence\n",
        "def plot_wordcount(count_ones_length,count_zeros_length):\n",
        "    fig,(ax1,ax2)=plt.subplots(1,2,figsize=(15,5))\n",
        "    sns.distplot(count_zeros_length,ax=ax1,color='Blue')\n",
        "    ax1.set_title('Sincere Question Length')\n",
        "    sns.distplot(count_ones_length,ax=ax2,color='Red')\n",
        "    ax2.set_title('Insincere Question Length')\n",
        "    fig.suptitle('Average Length of Words in Question')\n",
        "    plt.show()\n",
        "\n",
        "#Generic Plotter\n",
        "def plot_count(count_punct_ones,count_punct_zeros,title_1,title_2,subtitle):\n",
        "    fig,(ax1,ax2)=plt.subplots(1,2,figsize=(15,5))\n",
        "    sns.distplot(count_punct_zeros,ax=ax1,color='Blue')\n",
        "    ax1.set_title(title_1)\n",
        "    sns.distplot(count_punct_ones,ax=ax2,color='Red')\n",
        "    ax2.set_title(title_2)\n",
        "    fig.suptitle(subtitle)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "#preliminary word cloud statistics\n",
        "def display_cloud(data):\n",
        "    stopwords=set(STOPWORDS)\n",
        "    wordcloud=WordCloud(stopwords=stopwords,max_font_size=120,max_words=300,width=800,height=400,background_color='white',min_font_size=5).generate(str(data))\n",
        "    plt.figure(figsize=(24,8))\n",
        "    plt.imshow(wordcloud)\n",
        "    plt.axis(\"off\")\n",
        "    plt.title(\"Word Cloud of the questions\")\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#Extract the length of the insincere-sincere questions\n",
        "def word_length(x):\n",
        "    return len(x)\n",
        "\n",
        "\n",
        "count_ones_length=train_df[train_df['target']==1]['question_text'].str.split().apply(lambda z:word_length(z))\n",
        "print(\"Length of each insincere questions\".format(),count_ones_length[:5])\n",
        "count_zeros_length=train_df[train_df['target']==0]['question_text'].str.split().apply(lambda z: word_length(z))\n",
        "print(\"Length of each sincere questions\".format(),count_zeros_length[:5])\n",
        "\n",
        "#Plots\n",
        "plot_counts(count_ones,count_zeros)\n",
        "plot_wordcount(count_ones_length,count_zeros_length)\n",
        "display_cloud(train_df['question_text'])\n",
        "display_cloud(train_df[train_df['target']==1]['question_text'])\n",
        "display_cloud(train_df[train_df['target']==0]['question_text'])\n",
        "#Other analysis\n",
        "stops=set(stopwords.words('english'))\n",
        "count_punct_ones=train_df[train_df['target']==1]['question_text'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]))\n",
        "count_punct_zeros=train_df[train_df['target']==0]['question_text'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]))\n",
        "title_1='Sincere Question Punctuations'\n",
        "title_2='Insincere Question Punctuations'\n",
        "subtitle='Punctuations in Questions'\n",
        "plot_count(count_punct_ones,count_punct_zeros,title_1,title_2,subtitle)\n",
        "\n",
        "count_avg_ones=train_df[train_df['target']==1]['question_text'].apply(lambda z : np.mean([len(z) for w in str(z).split()]))\n",
        "count_avg_zeros=train_df[train_df['target']==0]['question_text'].apply(lambda z: np.mean([len(z) for w in str(z).split()]))\n",
        "title_1='Insincere Question Average Length'\n",
        "title_2='Sincere Question Average Length'\n",
        "subtitle='Average Length'\n",
        "plot_count(count_avg_ones,count_avg_zeros,title_1,title_2,subtitle)"
      ],
      "metadata": {
        "trusted": true,
        "id": "pLImcwVmWwFg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Test dataset Word Statistics\n",
        "\n",
        "def plot_testcount(count_test,title1,subtitle):\n",
        "    fig,(ax1)=plt.subplots(1,figsize=(15,5))\n",
        "    sns.distplot(count_test,ax=ax1,color='Orange')\n",
        "    ax1.set_title(title_1)\n",
        "    fig.suptitle(subtitle)\n",
        "    plt.show()\n",
        "\n",
        "count_test=test_df['question_text'].str.split().apply(lambda z:word_length(z))\n",
        "print(\"Length of each test questions\".format(),count_test[:5])\n",
        "\n",
        "#Plots\n",
        "plot_testcount(count_test,'Questions in test','Total Questions')\n",
        "display_cloud(test_df['question_text'])\n",
        "#Other analysis\n",
        "stops=set(stopwords.words('english'))\n",
        "count_puncttest=test_df['question_text'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]))\n",
        "title_1='Test Question Punctuations'\n",
        "subtitle='Punctuations in Questions'\n",
        "plot_testcount(count_puncttest,title_1,subtitle)\n",
        "\n",
        "count_avg_test=test_df['question_text'].apply(lambda z : np.mean([len(z) for w in str(z).split()]))\n",
        "title_1='Test Question Average Length'\n",
        "subtitle='Average Length'\n",
        "plot_testcount(count_avg_test,title_1,subtitle)"
      ],
      "metadata": {
        "trusted": true,
        "id": "soelLJGhWwFg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Analysis 2- Gram Statistics\n",
        "\n",
        "This will be useful for analysing the gram (bi,tri) in the words in the questions. It is an important step before cleaning.\n",
        "Resources:\n",
        "\n",
        "1. [Seaborn](https://seaborn.pydata.org/index.html)\n",
        "2. [Gram Analysis](https://albertauyeung.github.io/2018/06/03/generating-ngrams.html)\n",
        "\n",
        "A pictorial representation of gram analysis is provided here:\n",
        "\n",
        "<img src=\"https://miro.medium.com/max/536/1*vZhxrBkCz-yN_rzZBqSKiA.png\"></img>"
      ],
      "metadata": {
        "id": "fJxbbniOWwFg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Gram analysis on Training set\n",
        "stopword=set(stopwords.words('english'))\n",
        "def gram_analysis(data,gram):\n",
        "    tokens=[t for t in data.lower().split(\" \") if t!=\"\" if t not in stopword]\n",
        "    ngrams=zip(*[tokens[i:] for i in range(gram)])\n",
        "    final_tokens=[\" \".join(z) for z in ngrams]\n",
        "    return final_tokens\n",
        "\n",
        "#analyse most common Sentences\n",
        "def mostcommon_words(data):\n",
        "    counter=Counter(data)\n",
        "    commonwords=counter.most_common()\n",
        "    x_coord,y_coord=[],[]\n",
        "    for words,occ in commonwords[:20]:\n",
        "        if words not in stopword:\n",
        "            x_coord.append(occ)\n",
        "            y_coord.append(words)\n",
        "\n",
        "    sns.barplot(x=x_coord,y=y_coord,saturation=1,orient=\"h\")\n",
        "\n",
        "#Create frequency grams for analysis\n",
        "\n",
        "def create_dict(data,grams):\n",
        "    freq_dict=defaultdict(int)\n",
        "    for sentence in data:\n",
        "        for tokens in gram_analysis(sentence,grams):\n",
        "            freq_dict[tokens]+=1\n",
        "    return freq_dict\n",
        "\n",
        "def horizontal_bar_chart(df, color):\n",
        "    trace = go.Bar(\n",
        "        y=df[\"n_gram_words\"].values[::-1],\n",
        "        x=df[\"n_gram_frequency\"].values[::-1],\n",
        "        showlegend=False,\n",
        "        orientation = 'h',\n",
        "        marker=dict(\n",
        "            color=color,\n",
        "        ),\n",
        "    )\n",
        "    return trace\n",
        "\n",
        "\n",
        "\n",
        "def create_new_df(freq_dict,):\n",
        "    freq_df=pd.DataFrame(sorted(freq_dict.items(),key=lambda z:z[1])[::-1])\n",
        "    freq_df.columns=['n_gram_words','n_gram_frequency']\n",
        "    #print(freq_df.head())\n",
        "    #plt.barh(freq_df['n_gram_words'][:20],freq_df['n_gram_frequency'][:20],linewidth=0.3)\n",
        "    #plt.show()\n",
        "    trace=horizontal_bar_chart(freq_df[:20],'blue')\n",
        "    return trace\n",
        "\n",
        "def plot_grams(trace_zero,trace_one):\n",
        "    fig = tools.make_subplots(rows=1, cols=2, vertical_spacing=0.04,\n",
        "                          subplot_titles=[\"Frequent words of sincere questions\",\n",
        "                                          \"Frequent words of insincere questions\"])\n",
        "    fig.append_trace(trace_zero, 1, 1)\n",
        "    fig.append_trace(trace_ones, 1, 2)\n",
        "    fig['layout'].update(height=1200, width=900, paper_bgcolor='rgb(233,233,233)', title=\"Word Count Plots\")\n",
        "    py.iplot(fig, filename='word-plots')\n",
        "\n",
        "\n",
        "train_df_zero=train_df[train_df['target']==0]['question_text']\n",
        "train_df_ones=train_df[train_df['target']==1]['question_text']\n",
        "\n",
        "#mostcommon_words(train_df_zero)\n",
        "print(\"Bi-gram analysis\")\n",
        "freq_train_df_zero=create_dict(train_df_zero[:200],2)\n",
        "#print(freq_train_df_zero)\n",
        "trace_zero=create_new_df(freq_train_df_zero)\n",
        "freq_train_df_ones=create_dict(train_df_ones[:200],2)\n",
        "#print(freq_train_df_zero)\n",
        "trace_ones=create_new_df(freq_train_df_ones)\n",
        "plot_grams(trace_zero,trace_ones)\n",
        "\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "vz4Cp1S1WwFg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Tri-gram analysis\")\n",
        "freq_train_df_zero=create_dict(train_df_zero[:200],3)\n",
        "#print(freq_train_df_zero)\n",
        "trace_zero=create_new_df(freq_train_df_zero)\n",
        "freq_train_df_ones=create_dict(train_df_ones[:200],3)\n",
        "#print(freq_train_df_zero)\n",
        "trace_ones=create_new_df(freq_train_df_ones)\n",
        "plot_grams(trace_zero,trace_ones)\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "__pTdBbSWwFg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Sentence Analysis\n",
        "mostcommon_words(train_df_zero)"
      ],
      "metadata": {
        "trusted": true,
        "id": "wW1FOggMWwFh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Sentence Analysis\n",
        "mostcommon_words(train_df_ones)"
      ],
      "metadata": {
        "trusted": true,
        "id": "FV2wQkpFWwFh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Similar gram analysis using nltk after regexing\n",
        "\n",
        "import re\n",
        "from nltk.util import ngrams\n",
        "def gram_analyse(s):\n",
        "\n",
        "    s = s.lower()\n",
        "    s = re.sub(r'[^a-zA-Z0-9\\s]', ' ', s)\n",
        "    tokens = [token for token in s.split(\" \") if token != \"\"]\n",
        "    output = list(ngrams(tokens, 5))\n",
        "    return output\n",
        "\n",
        "for j in range(0,10):\n",
        "\n",
        "    out=gram_analyse(train_df_ones.iloc[j])\n",
        "    print(\"Output grams\",out)\n",
        "\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "FedQE-YHWwFh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Inference From Initial Word Analysis\n",
        "\n",
        "From the analysis, we see Sincere questions are in a larger quantity as compared to the Insincere questions. This may lead us to think the dataset is imbalanced. The gram statistics shows that there is a good correlation in trigram analysis of Insincere questions as compared to the Sincere ones. The number of words in insincere questions is larger as compared to the sincere ones, and the same can be deduced from the average sentence length of them as well.\n",
        "\n",
        "For balancing certain strategies are important such as synthetic oversampling\n",
        "\n",
        "1. [SMOTE](https://imbalanced-learn.readthedocs.io/en/stable/generated/imblearn.over_sampling.SMOTE.html)\n",
        "2. [ADASYN](https://imbalanced-learn.readthedocs.io/en/stable/generated/imblearn.over_sampling.ADASYN.html)\n",
        "\n",
        "\n",
        "\n",
        "There are other undersampling and oversampling methods as well:\n",
        "\n",
        "1. [UnderSampling](https://imbalanced-learn.readthedocs.io/en/stable/under_sampling.html)\n",
        "2. [OverSampling](https://imbalanced-learn.readthedocs.io/en/stable/over_sampling.html)\n",
        "\n",
        "This cannot be directly applied to textual data as it can be applied to vectorized(float) data.\n",
        "\n",
        "\n",
        "SMOTE for imbalanced data can be previewed as follows (with orange samples being the synthetically generated class):\n",
        "\n",
        "<img src=\"data:image/png;base64,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\"></img>\n",
        "\n",
        "Synthetic Data points generated by density functions in Adasyn is represented below:\n",
        "\n",
        "<img src=\"https://glemaitre.github.io/imbalanced-learn/_images/sphx_glr_plot_adasyn_001.png\"></img>\n"
      ],
      "metadata": {
        "id": "mZUzFzAXWwFh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Theoretical Diagram of SMOTE/ADASYN operation\n",
        "\n",
        "Smote and Adasyn employ distance metrics of KNN/Kmeans for generating synthetic points. The difference lies in how the points are generated- Smote employs weighted distance metrics whereas Adasyn performs probabilistic density distributions for generating new points.\n",
        "\n",
        "<img src=\"data:image/png;base64,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\"></img>"
      ],
      "metadata": {
        "id": "5yXz_NpmWwFh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Some cleaning\n",
        "\n",
        "This is mainly used for removing certain punctuations, stopwords from the corpus. Though the data is quite clean, we will clean this further with regex.\n",
        "\n",
        "1. [Regex](https://docs.python.org/3/library/re.html)\n",
        "\n",
        "We can also stem the corpus but that will not be helpful in the long run, though lemmatizing may  be helpful to a certain extent.\n",
        "\n",
        "1. [Stem](https://www.nltk.org/howto/stem.html)\n",
        "2. [Lemmatize](http://www.nltk.org/api/nltk.stem.html?highlight=lemmatizer)\n"
      ],
      "metadata": {
        "id": "B4ek9JQWWwFi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Regex cleaning\n",
        "\n",
        "def remove_punctuations(data):\n",
        "    punct_tag=re.compile(r'[^\\w\\s]')\n",
        "    data=punct_tag.sub(r'',data)\n",
        "    return data\n",
        "def remove_html(data):\n",
        "    html_tag=re.compile(r'<.*?>')\n",
        "    data=html_tag.sub(r'',data)\n",
        "    return data\n",
        "\n",
        "def remove_url(data):\n",
        "    url_clean= re.compile(r\"https://\\S+|www\\.\\S+\")\n",
        "    data=url_clean.sub(r'',data)\n",
        "    return data\n",
        "def clean_data(data):\n",
        "    emoji_clean= re.compile(\"[\"\n",
        "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "                           u\"\\U00002702-\\U000027B0\"\n",
        "                           u\"\\U000024C2-\\U0001F251\"\n",
        "                           \"]+\", flags=re.UNICODE)\n",
        "    data=emoji_clean.sub(r'',data)\n",
        "    c\n",
        "    return data\n",
        "\n",
        "\n",
        "train_df['question_text']=train_df['question_text'].apply(lambda z : remove_url(z))\n",
        "train_df['question_text']=train_df['question_text'].apply(lambda z: clean_data(z))\n",
        "train_df['question_text']=train_df['question_text'].apply(lambda z: remove_html(z))\n",
        "train_df['question_text']=train_df['question_text'].apply(lambda z: remove_punctuations(z))\n",
        "\n",
        "print(\"Cleaned Train Insincere Question Set\")\n",
        "print(train_df[train_df['target']==1]['question_text'].head())\n",
        "print(\"Cleaned Train Sincere Question Set\")\n",
        "print(train_df[train_df['target']==0]['question_text'].head())\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "mNrkzcLKWwFi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Lemmatizing the corpus as a backup\n",
        "\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "\n",
        "def lemma_traincorpus(data):\n",
        "    lemmatizer=WordNetLemmatizer()\n",
        "    out_data=\"\"\n",
        "    for words in data:\n",
        "        out_data+= lemmatizer.lemmatize(words)\n",
        "    return out_data\n",
        "\n",
        "train_df['question_text']=train_df['question_text'].apply(lambda z: lemma_traincorpus(z))\n",
        "print(train_df.head())"
      ],
      "metadata": {
        "trusted": true,
        "id": "lOGk14E1WwFi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Vectorization and Benchmarking\n",
        "\n",
        "In this context, we will look into vectorizing the training set with tfidf vectorizer and count vectorizer. Post that we will be fitting a statistical model from the sklearn library to get an initial benchmark . This is a very important step as an initial statistical benchmark should always be done before going in to deep models and advanced architectures.\n",
        "\n",
        "Here, we will be vectorizing the train corpus first and then we will be using the following models:\n",
        "\n",
        "1. [LogisticRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)\n",
        "2. [Naive Bayes](https://scikit-learn.org/stable/modules/naive_bayes.html)\n",
        "3. [SupportVectorMachines](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html)\n",
        "4. [LinearDiscriminantAnalysis](https://scikit-learn.org/stable/modules/generated/sklearn.discriminant_analysis.LinearDiscriminantAnalysis.html)\n",
        "5. [DecisionTrees](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html)\n",
        "6. [RandomForest](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html)\n",
        "7. [XGBoost](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html)\n",
        "8. [LightGBM](https://lightgbm.readthedocs.io/en/latest/pythonapi/lightgbm.LGBMClassifier.html)\n"
      ],
      "metadata": {
        "id": "xde-hkatWwFi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#tfidf vectorization\n",
        "\n",
        "tfidf_vect=TfidfVectorizer(stop_words='english',ngram_range=(1,3))\n",
        "#tfidf_vect.fit_transform(train_df['question_text'].values.tolist() + test_df['question_text'].values.tolist())\n",
        "train_tfidf=tfidf_vect.fit_transform(train_df['question_text'].values.tolist())\n",
        "test_tfidf=tfidf_vect.fit_transform(test_df['question_text'].values.tolist())\n",
        "print(train_tfidf)\n",
        "\n",
        "#count vectorization\n",
        "def count_vectorize(train_data,test_data):\n",
        "    count_vectorize=CountVectorizer(stop_words='english',ngram_range=(1,3),analyzer='word',token_pattern='r\\w{1,}')\n",
        "    count_vectorize.fit_transform(train_data['question_text'].values.tolist() + test_data['question_text'].values.tolist())\n",
        "    train_count=count_vectorize.transform(train_data['question_text'].values.tolist())\n",
        "    test_count=count_vectorize.transform(test_data['question_text'].values.tolist())\n",
        "    return train_count,test_count\n",
        "\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "winm_L0sWwFi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating Statistical Models\n",
        "\n",
        "Here, we will be analysing the outcomes of different models mentioned above using k fold statistics from sklearn.\n",
        "\n",
        "1. [KFold](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html)\n",
        "2. [StratifiedKFold](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedKFold.html)\n",
        "3. [Kernel Using KFold](https://www.kaggle.com/abhilash1910/credit-card-fraud)"
      ],
      "metadata": {
        "id": "1WZrQ3oYWwFi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_y=train_df['target']\n",
        "\n",
        "train_x=train_df['question_text']\n",
        "print(train_y)\n",
        "print(train_x)"
      ],
      "metadata": {
        "trusted": true,
        "id": "uKmsqzltWwFi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Some preliminary models for sequential evaluation on the training set\n",
        "\n",
        "#train_y=train_df['target'].values\n",
        "\n",
        "# models=[]\n",
        "# models.append(('LR',LogisticRegression()))\n",
        "# models.append(('KNN',KNeighborsClassifier()))\n",
        "# models.append(('LDA',LinearDiscriminantAnalysis()))\n",
        "# models.append(('DT',DecisionTreeClassifier()))\n",
        "# #models.append(('SVC',SVC()))\n",
        "# model_result=[]\n",
        "# scoring='accuracy'\n",
        "# for name,model in models:\n",
        "#     kfold=KFold(n_splits=10,random_state=7)\n",
        "#     results=cross_val_score(model,train_tfidf,train_y,cv=kfold,scoring=scoring)\n",
        "#     print(\"Classifiers: \",name, \"Has a training score of\", round(results.mean(), 2) * 100, \"% accuracy score\")\n",
        "#     model_result.append(results.mean())"
      ],
      "metadata": {
        "trusted": true,
        "id": "B94yWIODWwFi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "2dVDF-aCWwFi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Partitioning the Set\n",
        "\n",
        "Since there is no validation set to benchmark against , we create an internal partition of training and testing sets from the tfidf transformed data.\n",
        "\n",
        "1.[TrainTestSplit](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html)"
      ],
      "metadata": {
        "id": "FeqTyAHhWwFi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#split the training set into train and test evaluation sets\n",
        "print(train_tfidf.shape)\n",
        "\n",
        "train_x,test_x,train_y,test_y=train_test_split(train_tfidf,train_y,test_size=0.2,random_state=42)\n",
        "print(train_x.shape)\n",
        "print(train_y.shape)\n",
        "print(test_x.shape)\n",
        "print(test_y.shape)"
      ],
      "metadata": {
        "trusted": true,
        "id": "BSEw4yH_WwFj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# models=[]\n",
        "# #models.append(('LR',LogisticRegression()))\n",
        "# models.append(('KNN',KNeighborsClassifier()))\n",
        "# models.append(('LDA',LinearDiscriminantAnalysis()))\n",
        "# models.append(('DT',DecisionTreeClassifier()))\n",
        "\n",
        "# #Now trying out different stats models\n",
        "# def model_training(model,train_x,test_x,train_y,test_y):\n",
        "#     model.fit(train_x,train_y)\n",
        "#     pred=model.predict(test_x)\n",
        "#     print(\"Evaluate confusion matrix\")\n",
        "#     print(confusion_matrix(test_y,pred))\n",
        "#     print(accuracy_score(test_y,pred))\n",
        "#     return accuracy_score(test_y,pred)\n",
        "\n",
        "# for name,mods in models:\n",
        "#     accuracy_score=model_training(mods,train_x,test_x,train_y,test_y)\n",
        "#     print(accuracy_score)\n",
        "\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "Q9pBvXWQWwFj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Logistic Regression\n",
        "\n",
        "The logistic regression relies on Sigmoid function for classifications:\n",
        "\n",
        "<img src=\"https://res.cloudinary.com/practicaldev/image/fetch/s--xoKf0Xfi--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://cdn-images-1.medium.com/max/2000/1%2AXisqJi764DTxragiRFexSQ.png\"></img>"
      ],
      "metadata": {
        "id": "aNAMy1STWwFj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Logistic Regression\n",
        "model=LogisticRegression()\n",
        "model.fit(train_x,train_y)\n",
        "pred=model.predict(test_x)\n",
        "print(\"Evaluate confusion matrix\")\n",
        "print(confusion_matrix(test_y,pred))\n",
        "print(accuracy_score(test_y,pred))\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "bigsgtJfWwFj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "4vs11lTnWwFj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Naive Bayes\n",
        "\n",
        "model=MultinomialNB()\n",
        "model.fit(train_x,train_y)\n",
        "pred=model.predict(test_x)\n",
        "print(\"Evaluate confusion matrix\")\n",
        "print(confusion_matrix(test_y,pred))\n",
        "print(accuracy_score(test_y,pred))\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "6f7brpFfWwFj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LDA OverView:\n",
        "\n",
        "A Generic LDA, can be viewed as follows:\n",
        "\n",
        "<img src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_lda_qda_0011.png\"></img>"
      ],
      "metadata": {
        "id": "RrybcMoVWwFj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#LDA- Linear Disciminant Analysis\n",
        "\n",
        "model=LinearDiscriminantAnalysis()\n",
        "x_train=train_x.toarray()\n",
        "model.fit(x_train,train_y)\n",
        "pred=model.predict(test_x)\n",
        "print(\"Evaluate confusion matrix\")\n",
        "print(confusion_matrix(test_y,pred))\n",
        "print(accuracy_score(test_y,pred))\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "noGFEPQ-WwFj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Idea on Boosted Trees Architecture\n",
        "\n",
        "This [link](https://xgboost.readthedocs.io/en/latest/tutorials/model.html) provides a good material for boosted trees , which can be shown below as :\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/dmlc/web-data/master/xgboost/model/struct_score.png\"></img>"
      ],
      "metadata": {
        "id": "LiRakhY1WwFj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#XGBoost\n",
        "from xgboost import XGBClassifier as xg\n",
        "model_xgb= xg(n_estimators=100,random_state=42)\n",
        "model_xgb.fit(train_x,train_y)\n",
        "y_pred_xgb=model_xgb.predict(test_x)\n",
        "print(\"Confusion matrix\")\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "jIShevFjWwFj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Boosting Architecture Summary\n",
        "\n",
        "This provides a proper outline of a generic boosting model:\n",
        "\n",
        "<img src=\"https://miro.medium.com/max/1554/1*FLshv-wVDfu-i54OqvZdHg.png\"></img>"
      ],
      "metadata": {
        "id": "hcXc5K-FWwFk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(confusion_matrix(test_y,y_pred_xgb))\n",
        "print(accuracy_score(test_y,y_pred_xgb.round()))"
      ],
      "metadata": {
        "trusted": true,
        "id": "cUZlnZwtWwFk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#LightGBM\n",
        "from lightgbm import LGBMClassifier as lg\n",
        "model_lgbm= lg(n_estimators=100,random_state=42)\n",
        "model_lgbm.fit(train_x,train_y)\n",
        "y_pred_lgbm=model_lgbm.predict(test_x)\n",
        "print(\"Confusion matrix\")\n",
        "print(confusion_matrix(test_y,y_pred_lgbm))\n",
        "print(accuracy_score(test_y,y_pred_lgbm.round()))"
      ],
      "metadata": {
        "trusted": true,
        "id": "UcrAXQOZWwFk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Decision Trees\n",
        "model_dt=DecisionTreeClassifier(random_state=42)\n",
        "model_dt.fit(train_x,train_y)\n",
        "y_pred=model_dt.predict(test_x)\n",
        "print(\"Confusion matrix\")\n",
        "print(confusion_matrix(test_y,y_pred))\n",
        "print(accuracy_score(test_y,y_pred.round()))\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "Ibc_a2YhWwFk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Random Forest Architecture\n",
        "\n",
        "A random forest architecture with majority voting mechanism appears as follows:\n",
        "\n",
        "<img src=\"https://www.researchgate.net/profile/Hung_Cao12/publication/333438248/figure/fig6/AS:763710377299970@1559094151459/Random-Forest-model-with-majority-voting.ppm\"></img>"
      ],
      "metadata": {
        "id": "-jZNbrlfWwFk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Random Forest\n",
        "model_dt=RandomForestClassifier(random_state=42)\n",
        "model_dt.fit(train_x,train_y)\n",
        "y_pred=model_dt.predict(test_x)\n",
        "print(\"Confusion matrix\")\n",
        "print(confusion_matrix(test_y,y_pred))\n",
        "print(accuracy_score(test_y,y_pred.round()))\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "kXTN5rFJWwFk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inference From Statistical Models\n",
        "\n",
        "We see that statistical models provide a good benchmark  in terms of performance. Hyperparamater tuning using gridsearch,random search can also help to increase the performance.\n",
        "\n",
        "1. [GridSearch](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html)\n",
        "2. [RandomizedSearch](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html)\n",
        "\n",
        "We will also train SVM on the dataset. Since SVM generally takes a lot of time to train and hence dimensionality reduction is very important before using SVM for statistical modelling.\n"
      ],
      "metadata": {
        "id": "1MAdqrDJWwFk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dimensionality Reduction Techniques\n",
        "\n",
        "Conducting PCA, SVD and TSNE transformation of the input vectors to generate finite central points . These techniques involve statistical tensor/matrix decomposition algorithms which try to preserve the features based on the importance of the eigen vectors of the matrix."
      ],
      "metadata": {
        "id": "oZCNynOfWwFk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def dimen_reduc_plot(test_data,test_label,option):\n",
        "    tsvd= TruncatedSVD(n_components=2,algorithm=\"randomized\",random_state=42)\n",
        "    tsne=TSNE(n_components=2,random_state=42) #not recommended instead use PCA\n",
        "    pca=SparsePCA(n_components=2,random_state=42)\n",
        "    if(option==1):\n",
        "        tsvd_result=tsvd.fit_transform(test_data)\n",
        "        plt.figure(figsize=(10,8))\n",
        "        colors=['orange','red']\n",
        "\n",
        "        sns.scatterplot(x=tsvd_result[:,0],y=tsvd_result[:,1],hue=test_label        )\n",
        "\n",
        "        plt.show()\n",
        "        plt.figure(figsize=(10,10))\n",
        "        plt.scatter(tsvd_result[:,0],tsvd_result[:,1],c=test_label,cmap=matplotlib.colors.ListedColormap(colors))\n",
        "        color_red=mpatches.Patch(color='red',label='False_Tweet')\n",
        "        color_orange=mpatches.Patch(color='orange',label='Real_Tweet')\n",
        "        plt.legend(handles=[color_orange,color_red])\n",
        "        plt.title(\"TSVD\")\n",
        "        plt.show()\n",
        "    if(option==2):\n",
        "        tsne_result=tsne.fit_transform(test_data)\n",
        "        plt.figure(figsize=(10,8))\n",
        "        colors=['orange','red']\n",
        "        sns.scatterplot(x=tsne_result[:,0],y=tsne_result[:,1],hue=test_label)\n",
        "        plt.show()\n",
        "        plt.figure(figsize=(10,10))\n",
        "        plt.scatter(x=tsne_result[:,0],y=tsne_result[:,1],c=test_label,cmap=matplotlib.colors.ListedColormap(colors))\n",
        "        color_red=mpatches.Patch(color='red',label='False_tweet')\n",
        "        color_orange=mpatches.Patch(color='orange',label='Real_Tweet')\n",
        "        plt.legend(handles=[color_orange,color_red])\n",
        "        plt.title(\"PCA\")\n",
        "        plt.show()\n",
        "    if(option==3):\n",
        "        pca_result=pca.fit_transform(test_data.toarray())\n",
        "        plt.figure(figsize=(10,8))\n",
        "        colors=['orange','red']\n",
        "        sns.scatterplot(x=pca_result[:,0],y=pca_result[:,1],hue=test_label)\n",
        "        plt.show()\n",
        "        plt.figure(figsize=(10,10))\n",
        "        plt.scatter(x=pca_result[:,0],y=pca_result[:,1],c=test_label,cmap=matplotlib.colors.ListedColormap(colors))\n",
        "        color_red=mpatches.Patch(color='red',label='False_tweet')\n",
        "        color_orange=mpatches.Patch(color='orange',label='Real_Tweet')\n",
        "        plt.legend(handles=[color_orange,color_red])\n",
        "        plt.title(\"TSNE\")\n",
        "        plt.show()\n",
        "\n",
        "dimen_reduc_plot(train_x,train_y,1)\n",
        "dimen_reduc_plot(train_x,train_y,3)\n",
        "dimen_reduc_plot(train_x,train_y,2)\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "Nh9ZrUqnWwFk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def dimen_reduc_plot(test_data,test_label,option):\n",
        "    tsvd= TruncatedSVD(n_components=2,algorithm=\"randomized\",random_state=42)\n",
        "    tsne=TSNE(n_components=2,random_state=42) #not recommended instead use PCA\n",
        "    pca=SparsePCA(n_components=2,random_state=42)\n",
        "    if(option==1):\n",
        "        tsvd_result=tsvd.fit_transform(test_data)\n",
        "        plt.figure(figsize=(10,8))\n",
        "        colors=['orange','red']\n",
        "\n",
        "        sns.scatterplot(x=tsvd_result[:,0],y=tsvd_result[:,1],hue=test_label        )\n",
        "\n",
        "        plt.show()\n",
        "        plt.figure(figsize=(10,10))\n",
        "        plt.scatter(tsvd_result[:,0],tsvd_result[:,1],c=test_label,cmap=matplotlib.colors.ListedColormap(colors))\n",
        "        color_red=mpatches.Patch(color='red',label='False_Tweet')\n",
        "        color_orange=mpatches.Patch(color='orange',label='Real_Tweet')\n",
        "        plt.legend(handles=[color_orange,color_red])\n",
        "        plt.title(\"TSVD\")\n",
        "        plt.show()\n",
        "    if(option==2):\n",
        "        tsne_result=tsne.fit_transform(test_data)\n",
        "        plt.figure(figsize=(10,8))\n",
        "        colors=['orange','red']\n",
        "        sns.scatterplot(x=tsne_result[:,0],y=tsne_result[:,1],hue=test_label)\n",
        "        plt.show()\n",
        "        plt.figure(figsize=(10,10))\n",
        "        plt.scatter(x=tsne_result[:,0],y=tsne_result[:,1],c=test_label,cmap=matplotlib.colors.ListedColormap(colors))\n",
        "        color_red=mpatches.Patch(color='red',label='False_tweet')\n",
        "        color_orange=mpatches.Patch(color='orange',label='Real_Tweet')\n",
        "        plt.legend(handles=[color_orange,color_red])\n",
        "        plt.title(\"PCA\")\n",
        "        plt.show()\n",
        "    if(option==3):\n",
        "        pca_result=pca.fit_transform(test_data.toarray())\n",
        "        plt.figure(figsize=(10,8))\n",
        "        colors=['orange','red']\n",
        "        sns.scatterplot(x=pca_result[:,0],y=pca_result[:,1],hue=test_label)\n",
        "        plt.show()\n",
        "        plt.figure(figsize=(10,10))\n",
        "        plt.scatter(x=pca_result[:,0],y=pca_result[:,1],c=test_label,cmap=matplotlib.colors.ListedColormap(colors))\n",
        "        color_red=mpatches.Patch(color='red',label='False_tweet')\n",
        "        color_orange=mpatches.Patch(color='orange',label='Real_Tweet')\n",
        "        plt.legend(handles=[color_orange,color_red])\n",
        "        plt.title(\"TSNE\")\n",
        "        plt.show()\n",
        "\n",
        "dimen_reduc_plot(test_x,test_y,1)\n",
        "dimen_reduc_plot(test_x,test_y,3)\n",
        "dimen_reduc_plot(test_x,test_y,2)\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "kwfhqxqaWwFk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Statistical Learning and Inference - Conclusion\n",
        "\n",
        "Through these methods, we have evaluated the importance of statistical models in nlp classification tasks just by sampling words, tokens .\n",
        "\n",
        "While these approaches are often overlooked upon , it is very important to mention that  these benchmarks are important to validate against deep learning models.\n",
        "  \n",
        "  [Abhishek Thakur's Kernel](https://www.kaggle.com/abhishek/approaching-almost-any-nlp-problem-on-kaggle) provides a good idea on this, and can be used for reference."
      ],
      "metadata": {
        "id": "KSvI9bziWwFl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Neural Networks\n",
        "\n",
        "Building a preliminary deep model using sophisticated neural networks and variants of RNNs.\n",
        "I also built a simple LSTM model for validating the influence of deep models with respect to the statistical ones. In the first case,I will be using the Keras Embedding layer and visualize the results before using the embedding models.\n",
        "\n",
        "1. [Keras LSTM](https://keras.io/api/layers/recurrent_layers/lstm/)\n",
        "2. [Keras](https://keras.io/about/)\n",
        "3. [Keras Starter Guides](https://keras.io/examples/nlp/)\n",
        "4. [Tensorflow Starter](https://www.tensorflow.org/tutorials/keras/text_classification)\n",
        "5. [Tensorflow Hub](https://www.tensorflow.org/tutorials/keras/text_classification_with_hub)\n",
        "6. [Jason's Blog-Best practises](https://machinelearningmastery.com/best-practices-document-classification-deep-learning/)\n",
        "7. [Jason's Blog-Convolution Networks](https://machinelearningmastery.com/develop-word-embedding-model-predicting-movie-review-sentiment/)\n",
        "\n",
        "These are some starter resources for creating preliminary networks for sentiment analysis, text/intent classifications.\n",
        "\n",
        "LSTMs are a modified variant of RNNs having 4 gates (i,f,c,o) with the task of alleviating exploding and vanishing gradients.\n",
        "\n",
        "GRUs are another variant of LSTM cells with reduced gates.\n"
      ],
      "metadata": {
        "id": "zxwz1KULWwFl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Simple Dense Model with Embedding Architecture\n",
        "\n",
        "A simplified architecture of a neural network model with Embeddings and a MLP Dense Neuron is shown below:\n",
        "\n",
        "<img src=\"https://cdn-images-1.medium.com/max/800/1*4Uil1zWWF5-jlt-FnRJgAQ.png\"></img>"
      ],
      "metadata": {
        "id": "WtBwk_I6WwFl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating a Preliminary LSTM/CNN model\n",
        "\n",
        "In this stage, I will create a preliminary model without any sophisticated embeddings or architectures. I will be using the Keras Sequential API for creating our models. Let us follow the pipeline for creating any network for text classification:\n",
        "\n",
        "1. Tokenize the input features- This implies converting the input data into tokens (by using one hot encoding/tokenizing) .\n",
        "2. Tokenize the targets- This can be done with the help of Label Encoder(sklearn) or using \"values()\" from python\n",
        "3. Padd the tokenized features- To ensure that the length of the tokenized feature is same across all the entries(post padding)\n",
        "4. Create a simple model- Build a Sequential Network with Keras Embedding layer as the starting point\n",
        "5. Add Layers to the model- Add either Conv/LSTM/Bi-LSTM/RNN/GRU layers with different activations- relu is recommended\n",
        "6. Add the Dense layer to the model- At the end , we have to add the Dense layer by flattening the output of the previous layer\n",
        "7. Add necessary activation functions- Sigmoid for Binary Classification , Softmax for Multi-class classification\n",
        "8. Launch Tensorboard- Launch the Tensorboard and visualize the training parameters-loss,accuracy etc.\n",
        "\n",
        "This forms the fundamental steps to build a basic but fundamental pipeline for any language modelling task. Sophistications include adding custom embeddings before the keras Embedding layer and then adding certain other layers(transformer architectures) before the LSTM.\n"
      ],
      "metadata": {
        "id": "0fDIPoYsWwFl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sigmoid LSTM/Embedding Architecture\n",
        "\n",
        "This diagram provides an outline how the processing of words are done in LSTM - Embedding layers with Sigmoid activation:\n",
        "\n",
        "<img src=\"https://miro.medium.com/max/489/1*27JmK8VBdphpSCWNb4MhNA.png\"></img>"
      ],
      "metadata": {
        "id": "9Uvig5vWWwFl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.layers import LSTM, Dense,Flatten,Conv2D,Conv1D,GlobalMaxPooling1D\n",
        "from keras.optimizers import Adam\n",
        "#import MiniAttention.MiniAttention as ma\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "from bs4 import BeautifulSoup\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from nltk.corpus import stopwords\n",
        "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense, Concatenate, TimeDistributed, Bidirectional\n",
        "from tensorflow.keras.models import Model,Sequential\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.utils import to_categorical\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelBinarizer, LabelEncoder"
      ],
      "metadata": {
        "trusted": true,
        "id": "dy94caLYWwFl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the input features\n",
        "train_df=pd.read_csv('../input/quora-insincere-questions-classification/train.csv')\n",
        "test_df=pd.read_csv('../input/quora-insincere-questions-classification/test.csv')\n",
        "train_set,test_set=train_test_split(train_df,test_size=0.2,random_state=2017)\n",
        "print(train_set.shape)\n",
        "print(test_set.shape)"
      ],
      "metadata": {
        "trusted": true,
        "id": "F0zyGDkeWwFl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenize the Input Features\n",
        "\n",
        "In this stage we will be tokenizing the inputs using Keras Tokenizers."
      ],
      "metadata": {
        "id": "ix3kMvwIWwFl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "maxlen=1000\n",
        "max_features=5000\n",
        "embed_size=300\n",
        "\n",
        "#clean some null words or use the previously cleaned & lemmatized corpus\n",
        "\n",
        "train_x=train_set['question_text'].fillna('_na_').values\n",
        "val_x=test_set['question_text'].fillna('_na_').values\n",
        "\n",
        "#Tokenizing steps- must be remembered\n",
        "tokenizer=Tokenizer(num_words=max_features)\n",
        "tokenizer.fit_on_texts(list(train_x))\n",
        "train_x=tokenizer.texts_to_sequences(train_x)\n",
        "val_x=tokenizer.texts_to_sequences(val_x)\n",
        "\n",
        "#Pad the sequence- To allow same length for all vectorized words\n",
        "train_x=pad_sequences(train_x,maxlen=maxlen)\n",
        "val_x=pad_sequences(val_x,maxlen=maxlen)\n",
        "\n",
        "\n",
        "#get the target values - either using values or using Label Encoder\n",
        "train_y=train_set['target'].values\n",
        "val_y=test_set['target'].values"
      ],
      "metadata": {
        "trusted": true,
        "id": "HUUNW7aDWwFl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Padded and Tokenized Training Sequence\".format(),train_x.shape)\n",
        "print(\"Target Values Shape\".format(),train_y.shape)\n",
        "print(\"Padded and Tokenized Training Sequence\".format(),val_x.shape)\n",
        "print(\"Target Values Shape\".format(),val_y.shape)"
      ],
      "metadata": {
        "trusted": true,
        "id": "nSzxfeWAWwFm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_x[2])"
      ],
      "metadata": {
        "trusted": true,
        "id": "TOUdh6pqWwFm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating a basic Model\n",
        "\n",
        "In this step, we create a basic model with help of Keras Embedding Layer. Here we are using hte sequential API from Keras. Some resources related to the activation functions and layers:\n",
        "\n",
        "1. [Sequential](https://keras.io/guides/sequential_model/)\n",
        "2. [Tensorflow Activations](https://www.tensorflow.org/api_docs/python/tf/keras/activations)\n",
        "3. [Keras Loss](https://keras.io/api/losses/)\n",
        "4. [Keras Optimizers](https://keras.io/api/optimizers/)\n",
        "5. [Keras Metrics](https://keras.io/api/metrics/)\n",
        "6. [Dense Layer](https://keras.io/api/layers/core_layers/dense/)\n",
        "7. [Core Layers](https://keras.io/api/layers/core_layers/)\n",
        "\n",
        "These should be sufficient for getting started and creating models."
      ],
      "metadata": {
        "id": "wYMlauhsWwFm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Flow of layers in Sequential Models\n",
        "\n",
        "Particularly in NLP, the flow of the layers is as follows:\n",
        "\n",
        "1. Embedding Layer\n",
        "2. LSTM/CuDNNLSTM/ Bidirectional LSTM/CuDNNGRU/GRU layers\n",
        "2. Convolution/CNN\n",
        "3. GlobalMaxPooling Layer -Optional\n",
        "4. Dense Layer\n",
        "5. Final Dense Layer (with sigmoid activation for binary and softmax activation for multiclass)\n",
        "6. Model Compile - loss- binary_crossentropy (binary classification) or categorical_crossentropy(categorical classification)\n",
        "7. Optimizers for Model.compile - Adam ,Adagrad,Adadelta ,Nadam,Nesterov,SGD,RMSProp\n",
        "8. Metrics- accuracy\n",
        "9. Model fit - Input features\n",
        "\n",
        "This is the general flow for building any network in NLP\n",
        "\n"
      ],
      "metadata": {
        "id": "pBndtEvvWwFm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Creating a basic model- without pretrained embeddings\n",
        "model=Sequential()\n",
        "model.add(Embedding(max_features,embed_size,input_length=maxlen))\n",
        "model.add(LSTM(60))\n",
        "model.add(Dense(16,activation='relu'))\n",
        "model.add(Dense(1,activation='sigmoid'))\n",
        "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "metadata": {
        "trusted": true,
        "id": "V_cNuhqOWwFm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Fit the model with the inputs\n",
        "\n",
        "model.fit(train_x,train_y,batch_size=512,epochs=2,verbose=2)"
      ],
      "metadata": {
        "trusted": true,
        "id": "YI0MxLdnWwFm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#fit and validate together\n",
        "model.fit(train_x,train_y,batch_size=128,epochs=3,verbose=2,validation_data=(val_x,val_y))"
      ],
      "metadata": {
        "trusted": true,
        "id": "va81V7a2WwFm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating the model without using Sequential API\n",
        "\n",
        "In this case, we will be using a different technique to build models . We will specify the inputs explicitly at each layer of the models for this purpose.\n"
      ],
      "metadata": {
        "id": "iBjbkBbkWwFm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, Conv1D\n",
        "from keras.layers import Bidirectional, GlobalMaxPool1D\n",
        "from keras.models import Model\n",
        "from keras import initializers, regularizers, constraints, optimizers, layers\n",
        "import math\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "trusted": true,
        "id": "DfXYGe4cWwFm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade keras"
      ],
      "metadata": {
        "trusted": true,
        "id": "ZAQb7TqQWwFn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#CuDNN uses the Nvidia GPU for faster training\n",
        "\n",
        "inp=Input(shape=(maxlen,))\n",
        "z=Embedding(max_features,embed_size)(inp)\n",
        "z=Bidirectional(LSTM(60,return_sequences='True'))(z)\n",
        "z=GlobalMaxPool1D()(z)\n",
        "z=Dense(16,activation='relu')(z)\n",
        "z=Dense(1,activation='sigmoid')(z)\n",
        "model=Model(inputs=inp,outputs=z)\n",
        "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
        "model.summary()\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "YC_8iinkWwFn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#fit and validate together\n",
        "model.fit(train_x,train_y,batch_size=512,epochs=1,verbose=2,validation_data=(val_x,val_y))"
      ],
      "metadata": {
        "trusted": true,
        "id": "eNwC_FS3WwFn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "pj4p0SY0WwFn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Embeddings\n",
        "\n",
        "Now we will explore certain embeddings which may increase the performance of the model. Pre-trained embeddings provide a better representation of word vectors, and we will be analysing the word2vec, glove embeddigns as a starting point\n",
        "\n",
        "1. [Glove](https://nlp.stanford.edu/projects/glove/)\n",
        "2. [Word2Vec](https://radimrehurek.com/gensim/models/word2vec.html)\n",
        "\n",
        "\n",
        "Word2Vec employs cosine distance measure between word vectors:\n",
        "\n",
        "Global Vectors or glove also employs distance metric for comparison of the word vectors in an unsupervised manner:\n",
        "\n"
      ],
      "metadata": {
        "id": "hsVip3afWwFn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#see the input\n",
        "\n",
        "!ls ../input\n",
        "!ls ../input/quora-insincere-questions-classification"
      ],
      "metadata": {
        "trusted": true,
        "id": "5_F7TGUWWwFn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls ../input/wikinews300d1msubwordvec"
      ],
      "metadata": {
        "trusted": true,
        "id": "6BPcH-F2WwFn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "TMj3dA70WwFn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualization of Embeddings\n",
        "\n",
        "We can also visualize the embeddings which appears like a wave.\n",
        "A presentation of this is done in the notebook:\n",
        "\n",
        "1. [Twitter Analysis](https://www.kaggle.com/abhilash1910/tweet-analysis-eda-cleaning-tsne-glove-tf)\n"
      ],
      "metadata": {
        "id": "0LIblS-vWwFn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#visualize the embeddings\n",
        "plt.plot(embedding_matrix[10])\n",
        "plt.plot(embedding_matrix[5])"
      ],
      "metadata": {
        "trusted": true,
        "id": "WM3UiV1JWwFn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Rebuilding the model with pre-trained embeddings from Glove\n",
        "\n",
        "inp=Input(shape=(maxlen,))\n",
        "z=Embedding(max_features,embed_size,weights=[embedding_matrix])(inp)\n",
        "z=Bidirectional(LSTM(60,return_sequences='True'))(z)\n",
        "z=GlobalMaxPool1D()(z)\n",
        "z=Dense(16,activation='relu')(z)\n",
        "z=Dense(1,activation='sigmoid')(z)\n",
        "model=Model(inputs=inp,outputs=z)\n",
        "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
        "model.summary()\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "LHn1znNmWwFn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#fit and validate together- with glove embedding\n",
        "model.fit(train_x,train_y,batch_size=512,epochs=1,verbose=2,validation_data=(val_x,val_y))"
      ],
      "metadata": {
        "trusted": true,
        "id": "DlatStASWwFn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "#Fast Text Embeddings for training\n",
        "EMBEDDING_FILE = '../input/wikinews300d1msubwordvec/wiki-news-300d-1M-subword.vec'\n",
        "def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\n",
        "embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(EMBEDDING_FILE) if len(o)>100)\n",
        "\n",
        "all_embs = np.stack(embeddings_index.values())\n",
        "emb_mean,emb_std = all_embs.mean(), all_embs.std()\n",
        "embed_size = all_embs.shape[1]\n",
        "\n",
        "word_index = tokenizer.word_index\n",
        "nb_words = min(max_features, len(word_index))\n",
        "embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\n",
        "for word, i in word_index.items():\n",
        "    if i >= max_features: continue\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None: embedding_matrix[i] = embedding_vector\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "qtUzQ3SGWwFn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#visualize the embeddings-FastText\n",
        "plt.plot(embedding_matrix[10])\n",
        "plt.plot(embedding_matrix[5])"
      ],
      "metadata": {
        "trusted": true,
        "id": "w3d_JvYBWwFn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inp=Input(shape=(maxlen,))\n",
        "z=Embedding(max_features,embed_size,weights=[embedding_matrix])(inp)\n",
        "z=Bidirectional(LSTM(60,return_sequences='True'))(z)\n",
        "z=GlobalMaxPool1D()(z)\n",
        "z=Dense(16,activation='relu')(z)\n",
        "z=Dense(1,activation='sigmoid')(z)\n",
        "model=Model(inputs=inp,outputs=z)\n",
        "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
        "model.summary()\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "hwgkEo27WwFo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#fit and validate together- with glove embedding\n",
        "model.fit(train_x,train_y,batch_size=512,epochs=1,verbose=2,validation_data=(val_x,val_y))"
      ],
      "metadata": {
        "trusted": true,
        "id": "9fqyzcu6WwFo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! ls ../input\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "weFZd9cmWwFo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls ../input/googlenewsvectorsnegative300"
      ],
      "metadata": {
        "trusted": true,
        "id": "seII8Jo0WwFo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls ../input/paragram-300-sl999"
      ],
      "metadata": {
        "trusted": true,
        "id": "1dhdYPjtWwFo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! ls ../input/word2vec-sample"
      ],
      "metadata": {
        "trusted": true,
        "id": "5EwMWd21WwFo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls ../input"
      ],
      "metadata": {
        "trusted": true,
        "id": "_hkv0PvDWwFo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "#Paragram Embeddings for training\n",
        "EMBEDDING_FILE = '../input/paragram-300-sl999/paragram_300_sl999.txt'\n",
        "def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\n",
        "embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(EMBEDDING_FILE) if len(o)>100)\n",
        "\n",
        "all_embs = np.stack(embeddings_index.values())\n",
        "emb_mean,emb_std = all_embs.mean(), all_embs.std()\n",
        "embed_size = all_embs.shape[1]\n",
        "\n",
        "word_index = tokenizer.word_index\n",
        "nb_words = min(max_features, len(word_index))\n",
        "embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\n",
        "for word, i in word_index.items():\n",
        "    if i >= max_features: continue\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None: embedding_matrix[i] = embedding_vector\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "6OHljzeBWwFo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def add_corpus(data):\n",
        "    corpus=[]\n",
        "    for i in tqdm(data):\n",
        "        words=[word for word in (i)]\n",
        "        corpus.append(words)\n",
        "    return corpus\n",
        "def create_embedding(data):\n",
        "    embedding_map={}\n",
        "    file=open('../input/paragram-300-sl999/paragram_300_sl999.txt','r')\n",
        "    for  f in file:\n",
        "        values=f.split(' ')\n",
        "        word=values[0]\n",
        "        coef=np.asarray(values[1:],dtype='float32')\n",
        "        embedding_map[word]=coef\n",
        "    file.close()\n",
        "    return embedding_map\n",
        "def  embedding_preprocess(data):\n",
        "    #max_word_length=1000\n",
        "    max_sequence_length=100\n",
        "    tokenizer=Tokenizer()\n",
        "    tokenizer.fit_on_texts(data)\n",
        "    sequences=tokenizer.texts_to_sequences(data)\n",
        "\n",
        "    word_idx=tokenizer.word_index\n",
        "    data_pad=pad_sequences(sequences,padding=\"post\",maxlen=max_sequence_length)\n",
        "    emb_dim=data.get('a').shape[0]\n",
        "    num_length=len(word_idx)+1\n",
        "    emb_mat=np.zeros((num_length,emb_dim))\n",
        "    for word,idx in tqdm(word_idx.items()):\n",
        "        if idx > num_length:\n",
        "            continue\n",
        "        elif idx < num_length:\n",
        "            emb_vector=data.get(word)\n",
        "            if emb_vector is not None:\n",
        "                emb_mat[idx]=emb_vector\n",
        "\n",
        "    return emb_mat,word_idx,data_pad,num_length\n",
        "\n",
        "\n",
        "corpus_train_data=add_corpus(train_x)\n",
        "print(\"corpus created\")\n",
        "\n",
        "embedding_map= create_embedding(corpus_train_data)\n",
        "print(\"Embedding matrix created\")\n",
        "emb_mat,word_idx,pad_data,num_words=embedding_preprocess(embedding_map)\n",
        "print(pad_data.shape)\n",
        "\n",
        "print(\"Visualise embedded vectors\")\n",
        "plt.plot(emb_mat[10])\n",
        "plt.plot(emb_mat[20])\n",
        "plt.plot(emb_mat[50])\n",
        "plt.title(\"Embedding Vectors\")\n",
        "plt.show()"
      ],
      "metadata": {
        "trusted": true,
        "id": "yZKCuc0yWwFo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import KeyedVectors\n",
        "EMBEDDING_FILE = '../input/googlenewsvectorsnegative300/GoogleNews-vectors-negative300.bin'\n",
        "embeddings_index = KeyedVectors.load_word2vec_format(EMBEDDING_FILE, binary=True)\n",
        "\n",
        "word_index = tokenizer.word_index\n",
        "nb_words = min(max_features, len(word_index))\n",
        "embedding_matrix = (np.random.rand(nb_words, embed_size) - 0.5) / 5.0\n",
        "for word, i in word_index.items():\n",
        "    if i >= max_features: continue\n",
        "    if word in embeddings_index:\n",
        "        embedding_vector = embeddings_index.get_vector(word)\n",
        "        embedding_matrix[i] = embedding_vector"
      ],
      "metadata": {
        "trusted": true,
        "id": "iN-8z5grWwFo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inp=Input(shape=(maxlen,))\n",
        "z=Embedding(max_features,embed_size,weights=[embedding_matrix])(inp)\n",
        "z=Bidirectional(LSTM(60,return_sequences='True'))(z)\n",
        "z=GlobalMaxPool1D()(z)\n",
        "z=Dense(16,activation='relu')(z)\n",
        "z=Dense(1,activation='sigmoid')(z)\n",
        "model=Model(inputs=inp,outputs=z)\n",
        "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
        "model.summary()\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "xDC-WsubWwFo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#fit and validate together- with glove embedding\n",
        "model.fit(train_x,train_y,batch_size=512,epochs=1,verbose=2,validation_data=(val_x,val_y))"
      ],
      "metadata": {
        "trusted": true,
        "id": "lDGPiFraWwFo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Word2vec examples\n",
        "\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "model = Word2Vec.load_word2vec_format('../input/googlenewsvectorsnegative300/GoogleNews-vectors-negative300.bin', binary=True, norm_only=True)\n",
        "\n",
        "\n",
        "dog = model['dog']\n",
        "\n",
        "print(model.most_similar(positive=['woman', 'king'], negative=['man']))\n",
        "\n",
        "print(model.doesnt_match(\"breakfast cereal dinner lunch\".split()))\n",
        "\n",
        "print(model.similarity('woman', 'man'))"
      ],
      "metadata": {
        "trusted": true,
        "id": "Utapv9hzWwFo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Transformers"
      ],
      "metadata": {
        "id": "77l7jLl9a86K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I will be using variants of the BERT transformer.\n",
        "\n",
        "Firstly, I am using transformers made with the help of the [HuggingFace Repository](https://huggingface.co/) specifically the [DistillBert](https://huggingface.co/transformers/model_doc/distilbert.html) transformer.\n",
        "\n",
        "Transformers architecture is :\n",
        "\n",
        "<img src=\"https://i0.wp.com/esciencegroup.com/wp-content/uploads/2020/02/01.png?resize=506%2C641&ssl=1\"></img>"
      ],
      "metadata": {
        "id": "3LvO5Y-fbUyD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DistilBert\n",
        "\n",
        "In this case, the distilbert performs better than Bert in most cases owing to continuous feedback of attention weights from the teacher to the student network. Where the weights change by a large extent in case of Bert, this fails to happen in DistilBert."
      ],
      "metadata": {
        "id": "ZN80bESZbdiD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating DistillBert with Transformers\n",
        "\n",
        "We will be creating the DistillBert transformer and then training it with our own corpus.\n",
        "\n",
        "Then we will validate the results with our initial benchmarks from previous results.\n",
        "\n",
        "Follow this [link](https://huggingface.co/models) for testing any pre-trained transformer model."
      ],
      "metadata": {
        "id": "-n2DJuUZblu3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Dense, Input\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from kaggle_datasets import KaggleDatasets\n",
        "import transformers\n",
        "from tqdm.notebook import tqdm\n",
        "from tokenizers import BertWordPieceTokenizer\n",
        "from sklearn.model_selection import train_test_split\n"
      ],
      "metadata": {
        "id": "iFtZL7OSblh-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fk_l1prva2rr"
      },
      "cell_type": "markdown",
      "source": [
        "## Tokenize the Data (Fast Tokenize)\n",
        "\n",
        "This will assist in the model tuning stage. We have to divide the data in chunks so that it is simpler for the model to absorb the data."
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "_NCBLlw3a2rr"
      },
      "cell_type": "code",
      "source": [
        "#Tokenize the data and separate them in chunks of 256 units\n",
        "\n",
        "maxlen=512\n",
        "chunk_size=256\n",
        "def fast_encode(texts, tokenizer, chunk_size=chunk_size, maxlen=maxlen):\n",
        "    tokenizer.enable_truncation(max_length=maxlen)\n",
        "    tokenizer.enable_padding(max_length=maxlen)\n",
        "    all_ids = []\n",
        "    #sliding window methodology\n",
        "    for i in tqdm(range(0, len(texts), chunk_size)):\n",
        "        text_chunk = texts[i:i+chunk_size].tolist()\n",
        "        encs = tokenizer.encode_batch(text_chunk)\n",
        "        all_ids.extend([enc.ids for enc in encs])\n",
        "\n",
        "    return np.array(all_ids)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LHYqrGtKa2rr"
      },
      "cell_type": "markdown",
      "source": [
        "## Model Building With DistillBert\n",
        "\n",
        "Creating a function to help build the model,\n",
        "\n",
        "I will be using the [Model](https://www.kaggle.com/abhilash1910/nlp-workshop-ml-india/) API from Keras which was mentioned and addionally I will be using the same additional layers as Dense. The activations (sigmoid) also remain pretty much the same."
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "zBF64FWaa2rr"
      },
      "cell_type": "code",
      "source": [
        "# Create the model\n",
        "\n",
        "def build_model(transformer, max_len=512):\n",
        "    input_word_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n",
        "    #Replaced from the Embedding+LSTM/CoNN layers\n",
        "    sequence_output = transformer(input_word_ids)[0]\n",
        "    cls_token = sequence_output[:, 0, :]\n",
        "    out = Dense(1, activation='sigmoid')(cls_token)\n",
        "\n",
        "    model = Model(inputs=input_word_ids, outputs=out)\n",
        "    model.compile(Adam(lr=1e-5), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    model.summary()\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XFIQ2KHBa2rs"
      },
      "cell_type": "markdown",
      "source": [
        "## TPU Cluster Check\n",
        "\n",
        " I will be using the TPU cluster from the Notebook (Hardware accelerations). TPUs provide a better performance with respect to Tensorflow and Keras computations on tensors against GPUs. But it has to be explicitly called out in the code segment.\n",
        "\n",
        "[Kaggle Documentation on TPUs](https://www.kaggle.com/docs/tpu) provide an excellent starting point for this.Highly recommend to go through it.\n",
        "\n",
        "Steps to check and run the TPU cluster:\n",
        "\n",
        "### detect and init the TPU\n",
        "tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
        "tf.config.experimental_connect_to_cluster(tpu)\n",
        "tf.tpu.experimental.initialize_tpu_system(tpu)\n",
        "\n",
        "### instantiate a distribution strategy\n",
        "tpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
        "\n",
        "### instantiating the model in the strategy scope creates the model on the TPU\n",
        "with tpu_strategy.scope():\n",
        "    model = tf.keras.Sequential( … ) # define your model normally\n",
        "    model.compile( … )\n",
        "\n",
        "### train model normally\n",
        "model.fit(training_dataset, epochs=EPOCHS, steps_per_epoch=…)"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "JJj6SfPva2rs"
      },
      "cell_type": "code",
      "source": [
        "#Detect and deploy\n",
        "\n",
        "try:\n",
        "    # TPU detection. No parameters necessary if TPU_NAME environment variable is\n",
        "    # set: this is always the case on Kaggle.\n",
        "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
        "    print('Running on TPU ', tpu.master())\n",
        "except ValueError:\n",
        "    tpu = None\n",
        "\n",
        "if tpu:\n",
        "    tf.config.experimental_connect_to_cluster(tpu)\n",
        "    tf.tpu.experimental.initialize_tpu_system(tpu)\n",
        "    strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
        "else:\n",
        "    # Default distribution strategy in Tensorflow. Works on CPU and single GPU.\n",
        "    strategy = tf.distribute.get_strategy()\n",
        "\n",
        "print(\"REPLICAS: \", strategy.num_replicas_in_sync)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "W_D3OjI0a2rs"
      },
      "cell_type": "markdown",
      "source": [
        "## Replicas\n",
        "\n",
        "The replicas assist in segregating the data in sync, by allowing a faster batch sampling and an equal partition of the set, amongst the different replicas. In this context, it will be partitioned in blocks amongst 8 TPU clusters."
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "p1un_jFDa2rt"
      },
      "cell_type": "code",
      "source": [
        "#allow experimental tf\n",
        "AUTO = tf.data.experimental.AUTOTUNE\n",
        "\n",
        "# Data access\n",
        "GCS_DS_PATH = KaggleDatasets().get_gcs_path()\n",
        "\n",
        "# Configuration of hyperparameters\n",
        "EPOCHS = 3\n",
        "#batch size denotes the partitioning amongst the cluster replicas.\n",
        "BATCH_SIZE = 16 * strategy.num_replicas_in_sync\n",
        "MAX_LEN = 192"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TPR3CT5za2rt"
      },
      "cell_type": "markdown",
      "source": [
        "## Load the Tokenizer from DistillBert\n",
        "\n",
        "In this case, we will be using the [DistilBertTokenizer](https://huggingface.co/transformers/v2.11.0/model_doc/distilbert.html). This will help us to tokenize the data from 'distillbert-base-multilingual-cased' pre-trained tokenized model.\n",
        "\n",
        "The different tokenizers present in [HuggingFace](https://huggingface.co/transformers/main_classes/tokenizer.html)\n",
        "\n",
        "The two most important ones for generic language modelling:\n",
        "\n",
        "1. ByteLevelBPETokenizer\n",
        "2. BertWordPieceTokenizer\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "DaLBEjSua2rt"
      },
      "cell_type": "code",
      "source": [
        "# First load the real tokenizer\n",
        "tokenizer = transformers.DistilBertTokenizer.from_pretrained('distilbert-base-multilingual-cased')\n",
        "# Save the loaded tokenizer locally\n",
        "tokenizer.save_pretrained('.')\n",
        "# Reload it with the huggingface tokenizers library\n",
        "fast_tokenizer = BertWordPieceTokenizer('vocab.txt', lowercase=False)\n",
        "fast_tokenizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "KT2mZGMKa2ru"
      },
      "cell_type": "code",
      "source": [
        "train_df=pd.read_csv('../input/quora-insincere-questions-classification/train.csv')\n",
        "test_df=pd.read_csv('../input/quora-insincere-questions-classification/test.csv')\n",
        "train_set,test_set=train_test_split(train_df,test_size=0.2,random_state=2017)\n",
        "print(train_set.shape)\n",
        "print(test_set.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "l4305gbba2ru"
      },
      "cell_type": "code",
      "source": [
        "train_set['question_text'].shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "Y0y3naQBa2ru"
      },
      "cell_type": "code",
      "source": [
        "train_df['question_text']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "t7p-8eh9a2ru"
      },
      "cell_type": "markdown",
      "source": [
        "## Tokenize the text samples\n",
        "\n",
        "In this case, we apply the fast Tokenizer from Distillbert on the samples."
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "9l9wPJN8a2ru"
      },
      "cell_type": "code",
      "source": [
        "train_x = fast_encode(train_set['question_text'].astype(str), fast_tokenizer, maxlen=MAX_LEN)\n",
        "val_x = fast_encode(test_set['question_text'].astype(str), fast_tokenizer, maxlen=MAX_LEN)\n",
        "train_y=train_set['target'].values\n",
        "val_y=test_set['target'].values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "Hx48otbIa2rv"
      },
      "cell_type": "code",
      "source": [
        "print(train_x.shape)\n",
        "print(train_y.shape)\n",
        "print(val_x.shape)\n",
        "print(val_y.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wtge6C_8a2rv"
      },
      "cell_type": "markdown",
      "source": [
        "## Create Datasets\n",
        "\n",
        "An important aspect of finetuning Bert variants of transformers is to use a proper tensor slicing mechanism for splitting the training and validation sets. This allows the algorithm to understand the different datasets. This also allows the DistillBert algorithm to download and train the data in the form of \"tf.data.Dataset\", which effectively implies that we have converted the dataset to be compatible with tensorflow datasets."
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "oQjR7K5Ha2rv"
      },
      "cell_type": "code",
      "source": [
        "train_dataset = (\n",
        "    tf.data.Dataset\n",
        "    .from_tensor_slices((train_x, train_y))\n",
        "    .repeat()\n",
        "    .shuffle(2048)\n",
        "    .batch(BATCH_SIZE)\n",
        "    .prefetch(AUTO)\n",
        ")\n",
        "\n",
        "valid_dataset = (\n",
        "    tf.data.Dataset\n",
        "    .from_tensor_slices((val_x, val_y))\n",
        "    .batch(BATCH_SIZE)\n",
        "    .cache()\n",
        "    .prefetch(AUTO)\n",
        ")\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "7xS4aV9ja2rv"
      },
      "cell_type": "code",
      "source": [
        "print(train_dataset)\n",
        "print(valid_dataset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "VqHW78B7a2rw"
      },
      "cell_type": "code",
      "source": [
        "#Build the transformer model\n",
        "with strategy.scope():\n",
        "    transformer_layer = (\n",
        "        transformers.TFDistilBertModel\n",
        "        .from_pretrained('distilbert-base-multilingual-cased')\n",
        "    )\n",
        "    model = build_model(transformer_layer, max_len=MAX_LEN)\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "F5Wiu79Ua2rw"
      },
      "cell_type": "markdown",
      "source": [
        "## Train the Transformer\n",
        "\n",
        " train the DistillBert transformer with the corpus."
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "feZcdorfa2rx"
      },
      "cell_type": "code",
      "source": [
        "n_steps = train_x.shape[0] // BATCH_SIZE\n",
        "train_history = model.fit(\n",
        "    train_dataset,\n",
        "    steps_per_epoch=n_steps,\n",
        "    validation_data=valid_dataset,\n",
        "    epochs=EPOCHS\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "o1S1z1Dma2rx"
      },
      "cell_type": "markdown",
      "source": [
        "## Validating the Model\n",
        "\n",
        "After saturation of the training metrics(accuracy), we have to validate the model against the validation /testing set. This is similar to the code written above."
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "o0g2CAPQa2rx"
      },
      "cell_type": "code",
      "source": [
        "n_steps = valid_x.shape[0] // BATCH_SIZE\n",
        "train_history_2 = model.fit(\n",
        "    valid_dataset.repeat(),\n",
        "    steps_per_epoch=n_steps,\n",
        "    epochs=EPOCHS*2\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "o7MhkuIWa2rx"
      },
      "cell_type": "markdown",
      "source": [
        "## Outline for training with any Transformer\n",
        "\n",
        "The following is the outline for training any transformer(HuggingFace) with the Kaggle corpus:\n",
        "\n",
        "1. Create a function for Fast Tokenization. Provide the maxlen for truncating and chunk size.\n",
        "2. Create a function for the Transformer Model. Notice there are no additional Embedding/LSTM layers attached in the model.\n",
        "3. Detect and check for TPUs - this will greatly boost the training period for us.Tensor Processing Units are specifically designed    for super fast computing of the Tensors (in Tensorflow).\n",
        "4. Create Replicas to split the TPU cluster for better performance - recommended.\n",
        "5. Load the training and validation datasets into the Fast Tokenization function to encode it with any Transformer (in our case        DistillBert). Notice BertWordPieceTokenizer- this is used with any variant of Bert Transformer.\n",
        "6. After Tokenization, make the dataset compatible with Tensorflow datasets- (tensorflow.data.Datasets). This gives performance        boost on TPU.\n",
        "7. Load the Transformer model from The HuggingFace Repository. The code for this should be of the same pattern for any Transformer.\n",
        "8. Train the model with the parameters.\n",
        "9. Validate it against the validation set.\n",
        "\n",
        "This pipeline follows for any classification task with Transformers. For categorical classification, only the change is required in the model building (function) stage - the last Dense layer should have a softmax activation instead of sigmoid. Other than this, no changes are required.\n",
        "\n",
        "The list of pre-trained models from the HuggingFace repository can be found here:\n",
        "\n",
        "1. [Pre-trained Model Names](https://huggingface.co/transformers/pretrained_models.html)"
      ]
    },
    {
      "metadata": {
        "id": "liHnQT8Aa2rz"
      },
      "cell_type": "markdown",
      "source": [
        "## Albert- The lightweight Bert\n",
        "\n",
        "Testing the performance of ALbert from Google Research. It is a lightweight , which uses splits and tensor decomposition of the embedding matrix. Most of the architecture is same as Bert, with the addition of repeated layers which reduces memory consumption. These are the resources for understanding Albert and Bert:\n",
        "\n",
        "1. [HuggingFace Albert](https://huggingface.co/transformers/model_doc/albert.html)\n",
        "2. [Albert Paper](https://arxiv.org/abs/1909.11942)\n",
        "\n",
        "The original Bert paper and resources are present in these links:\n",
        "\n",
        "1. [Traditional BERT](https://huggingface.co/transformers/model_doc/bert.html)\n",
        "2. [Paper](https://arxiv.org/abs/1810.04805)\n",
        "\n",
        "Here I have first used Bert Transformer and then analyse the performance using Albert.\n",
        "This allowed me to see the speed up and the increase in performance in Albert from Bert.\n"
      ]
    },
    {
      "metadata": {
        "id": "3IHFwOGia2rz"
      },
      "cell_type": "markdown",
      "source": [
        "## Analysis with Bert Transformer\n",
        "\n",
        "First, let us build a composite Bert model for our use case.\n",
        "The model architecture of Bert is shown from the paper :\n",
        "\n",
        "<img src=\"https://miro.medium.com/max/1000/1*G6PYuBxc7ryP4Pz7nrZJgQ@2x.png\"></img>\n",
        "\n",
        "\n",
        "The pathway from pre-training to fine-tuning is shown here:\n",
        "\n",
        "<img src=\"data:image/jpeg;base64,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\"></img>\n"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "R_X9JDiCa2rz"
      },
      "cell_type": "code",
      "source": [
        "#Step 5.\n",
        "# First load the real tokenizer- Bert\n",
        "tokenizer = transformers.BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
        "# Save the loaded tokenizer locally\n",
        "tokenizer.save_pretrained('.')\n",
        "# Reload it with the huggingface tokenizers library\n",
        "fast_tokenizer = BertWordPieceTokenizer('vocab.txt', lowercase=False)\n",
        "fast_tokenizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "cipCeXD9a2rz"
      },
      "cell_type": "code",
      "source": [
        "\n",
        "train_df=pd.read_csv('../input/quora-insincere-questions-classification/train.csv')\n",
        "test_df=pd.read_csv('../input/quora-insincere-questions-classification/test.csv')\n",
        "train_set,test_set=train_test_split(train_df,test_size=0.2,random_state=2017)\n",
        "print(train_set.shape)\n",
        "print(test_set.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "7WcnAxvda2r0"
      },
      "cell_type": "code",
      "source": [
        "#Step 5.\n",
        "#Tokenize the datasets\n",
        "train_x = fast_encode(train_set['question_text'].astype(str), fast_tokenizer, maxlen=MAX_LEN)\n",
        "val_x = fast_encode(test_set['question_text'].astype(str), fast_tokenizer, maxlen=MAX_LEN)\n",
        "train_y=train_set['target'].values\n",
        "val_y=test_set['target'].values\n",
        "print(train_x.shape)\n",
        "print(train_y.shape)\n",
        "print(val_x.shape)\n",
        "print(val_y.shape)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "yIyAQTWDa2r0"
      },
      "cell_type": "code",
      "source": [
        "#Step 6.\n",
        "#Create Tensorflow Datasets\n",
        "train_dataset = (\n",
        "    tf.data.Dataset\n",
        "    .from_tensor_slices((train_x, train_y))\n",
        "    .repeat()\n",
        "    .shuffle(2048)\n",
        "    .batch(BATCH_SIZE)\n",
        "    .prefetch(AUTO)\n",
        ")\n",
        "\n",
        "valid_dataset = (\n",
        "    tf.data.Dataset\n",
        "    .from_tensor_slices((val_x, val_y))\n",
        "    .batch(BATCH_SIZE)\n",
        "    .cache()\n",
        "    .prefetch(AUTO)\n",
        ")\n",
        "\n",
        "print(train_dataset)\n",
        "print(valid_dataset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "Rzmr9FQYa2r0"
      },
      "cell_type": "code",
      "source": [
        "#Step 7.\n",
        "#Build the transformer model\n",
        "with strategy.scope():\n",
        "    transformer_layer = (\n",
        "        transformers.TFBertModel\n",
        "        .from_pretrained('bert-base-multilingual-cased')\n",
        "    )\n",
        "    model = build_model(transformer_layer, max_len=MAX_LEN)\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "-4WxbUvUa2r0"
      },
      "cell_type": "code",
      "source": [
        "#Step 8.\n",
        "#Train the model\n",
        "\n",
        "n_steps = train_x.shape[0] // BATCH_SIZE\n",
        "train_history = model.fit(\n",
        "    train_dataset,\n",
        "    steps_per_epoch=n_steps,\n",
        "    validation_data=valid_dataset,\n",
        "    epochs=EPOCHS\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "1LQjwguJa2r0"
      },
      "cell_type": "code",
      "source": [
        "#Step 9.\n",
        "#Validate the model\n",
        "\n",
        "n_steps = val_x.shape[0] // BATCH_SIZE\n",
        "train_history = model.fit(\n",
        "    train_dataset,\n",
        "    steps_per_epoch=n_steps,\n",
        "    validation_data=valid_dataset,\n",
        "    epochs=EPOCHS\n",
        ")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qDBypM0-a2r0"
      },
      "cell_type": "markdown",
      "source": [
        "## Analyse the Albert Model\n",
        "\n",
        "Now with the performance of Bert noted, I will be analysing the Albert model for this use case.\n",
        "For this, I have used the Albert Model from HuggingFace Transformers"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "Y5ZEGXK5a2r0"
      },
      "cell_type": "code",
      "source": [
        "#Step 5.\n",
        "# First load the real tokenizer- Bert\n",
        "tokenizer = transformers.AlbertTokenizer.from_pretrained('albert-base-v1')\n",
        "# Save the loaded tokenizer locally\n",
        "tokenizer.save_pretrained('.')\n",
        "# Reload it with the huggingface tokenizers library\n",
        "fast_tokenizer = BertWordPieceTokenizer('vocab.txt', lowercase=False)\n",
        "fast_tokenizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "gBF5oF5xa2r0"
      },
      "cell_type": "code",
      "source": [
        "\n",
        "train_df=pd.read_csv('../input/quora-insincere-questions-classification/train.csv')\n",
        "test_df=pd.read_csv('../input/quora-insincere-questions-classification/test.csv')\n",
        "train_set,test_set=train_test_split(train_df,test_size=0.2,random_state=2017)\n",
        "print(train_set.shape)\n",
        "print(test_set.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "vIzidJnpa2r1"
      },
      "cell_type": "code",
      "source": [
        "#Step 5.\n",
        "#Tokenize the datasets\n",
        "train_x = fast_encode(train_set['question_text'].astype(str), fast_tokenizer, maxlen=MAX_LEN)\n",
        "val_x = fast_encode(test_set['question_text'].astype(str), fast_tokenizer, maxlen=MAX_LEN)\n",
        "train_y=train_set['target'].values\n",
        "val_y=test_set['target'].values\n",
        "print(train_x.shape)\n",
        "print(train_y.shape)\n",
        "print(val_x.shape)\n",
        "print(val_y.shape)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "1Ktl_Gcfa2r1"
      },
      "cell_type": "code",
      "source": [
        "#Step 6.\n",
        "#Create Tensorflow Datasets\n",
        "train_dataset = (\n",
        "    tf.data.Dataset\n",
        "    .from_tensor_slices((train_x, train_y))\n",
        "    .repeat()\n",
        "    .shuffle(2048)\n",
        "    .batch(BATCH_SIZE)\n",
        "    .prefetch(AUTO)\n",
        ")\n",
        "\n",
        "valid_dataset = (\n",
        "    tf.data.Dataset\n",
        "    .from_tensor_slices((val_x, val_y))\n",
        "    .batch(BATCH_SIZE)\n",
        "    .cache()\n",
        "    .prefetch(AUTO)\n",
        ")\n",
        "\n",
        "print(train_dataset)\n",
        "print(valid_dataset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "YYcCJP5ja2r1"
      },
      "cell_type": "code",
      "source": [
        "#Step 7.\n",
        "#Build the transformer model\n",
        "with strategy.scope():\n",
        "    transformer_layer = (\n",
        "        transformers.TFAlbertModel\n",
        "        .from_pretrained('albert-base-v1')\n",
        "    )\n",
        "    model = build_model(transformer_layer, max_len=MAX_LEN)\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "If9beepPa2r1"
      },
      "cell_type": "code",
      "source": [
        "#Step 8.\n",
        "#Train the model\n",
        "\n",
        "n_steps = train_x.shape[0] // BATCH_SIZE\n",
        "train_history = model.fit(\n",
        "    train_dataset,\n",
        "    steps_per_epoch=n_steps,\n",
        "    validation_data=valid_dataset,\n",
        "    epochs=EPOCHS\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "smf205kra2r1"
      },
      "cell_type": "code",
      "source": [
        "#Step 9.\n",
        "#Validate the model\n",
        "\n",
        "n_steps = val_x.shape[0] // BATCH_SIZE\n",
        "train_history = model.fit(\n",
        "    train_dataset,\n",
        "    steps_per_epoch=n_steps,\n",
        "    validation_data=valid_dataset,\n",
        "    epochs=EPOCHS\n",
        ")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7j9XxyWNa2r1"
      },
      "cell_type": "markdown",
      "source": [
        "## GPT-2 transformer\n",
        "\n",
        "GPT(Generative Pre-training) is a SOTA for generative (transfer learning) based architecture which has achieved great heights with its new language models (GPT2/3). We will be analysing the performance of GPT-2 for our use case. Some important resources on GPT-2:\n",
        "\n",
        "1. [GPT-2](https://openai.com/blog/better-language-models/)\n",
        "2. [HuggingFace GPT-2](https://huggingface.co/transformers/model_doc/gpt2.html)\n"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "bm3tN_V_a2r1"
      },
      "cell_type": "code",
      "source": [
        "#Step 5.\n",
        "# First load the real tokenizer- GPT-2\n",
        "tokenizer = transformers.GPT2Tokenizer.from_pretrained('gpt2-medium')\n",
        "# Save the loaded tokenizer locally\n",
        "tokenizer.save_pretrained('.')\n",
        "# Reload it with the huggingface tokenizers library\n",
        "fast_tokenizer = BertWordPieceTokenizer('vocab.txt', lowercase=False)\n",
        "fast_tokenizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "dn75VUbha2r1"
      },
      "cell_type": "code",
      "source": [
        "\n",
        "train_df=pd.read_csv('../input/quora-insincere-questions-classification/train.csv')\n",
        "test_df=pd.read_csv('../input/quora-insincere-questions-classification/test.csv')\n",
        "train_set,test_set=train_test_split(train_df,test_size=0.2,random_state=2017)\n",
        "print(train_set.shape)\n",
        "print(test_set.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "eGzL68B0a2r2"
      },
      "cell_type": "code",
      "source": [
        "#Step 5.\n",
        "#Tokenize the datasets\n",
        "train_x = fast_encode(train_set['question_text'].astype(str), fast_tokenizer, maxlen=MAX_LEN)\n",
        "val_x = fast_encode(test_set['question_text'].astype(str), fast_tokenizer, maxlen=MAX_LEN)\n",
        "train_y=train_set['target'].values\n",
        "val_y=test_set['target'].values\n",
        "print(train_x.shape)\n",
        "print(train_y.shape)\n",
        "print(val_x.shape)\n",
        "print(val_y.shape)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "4vEU1DGaa2r2"
      },
      "cell_type": "code",
      "source": [
        "#Step 6.\n",
        "#Create Tensorflow Datasets\n",
        "train_dataset = (\n",
        "    tf.data.Dataset\n",
        "    .from_tensor_slices((train_x, train_y))\n",
        "    .repeat()\n",
        "    .shuffle(2048)\n",
        "    .batch(BATCH_SIZE)\n",
        "    .prefetch(AUTO)\n",
        ")\n",
        "\n",
        "valid_dataset = (\n",
        "    tf.data.Dataset\n",
        "    .from_tensor_slices((val_x, val_y))\n",
        "    .batch(BATCH_SIZE)\n",
        "    .cache()\n",
        "    .prefetch(AUTO)\n",
        ")\n",
        "\n",
        "print(train_dataset)\n",
        "print(valid_dataset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "t_Z38Vwca2r2"
      },
      "cell_type": "code",
      "source": [
        "#Step 7.\n",
        "#Build the transformer model\n",
        "with strategy.scope():\n",
        "    transformer_layer = (\n",
        "        transformers.TFGPT2Model\n",
        "        .from_pretrained('gpt2-medium')\n",
        "    )\n",
        "    model = build_model(transformer_layer, max_len=MAX_LEN)\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "0fQGLSSXa2r2"
      },
      "cell_type": "code",
      "source": [
        "#Step 8.\n",
        "#Train the model\n",
        "\n",
        "n_steps = train_x.shape[0] // BATCH_SIZE\n",
        "train_history = model.fit(\n",
        "    train_dataset,\n",
        "    steps_per_epoch=n_steps,\n",
        "    validation_data=valid_dataset,\n",
        "    epochs=EPOCHS\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "8K9S3n6Ea2r2"
      },
      "cell_type": "code",
      "source": [
        "#Step 9.\n",
        "#Validate the model\n",
        "\n",
        "n_steps = val_x.shape[0] // BATCH_SIZE\n",
        "train_history = model.fit(\n",
        "    train_dataset,\n",
        "    steps_per_epoch=n_steps,\n",
        "    validation_data=valid_dataset,\n",
        "    epochs=EPOCHS\n",
        ")\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}